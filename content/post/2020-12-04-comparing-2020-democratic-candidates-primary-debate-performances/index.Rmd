---
title: Comparing 2020 Democratic Candidates' Primary Debate Performances
author: Chad Peltier
date: '2020-12-04'
slug: comparing-2020-democratic-candidates-primary-debate-performances
categories:
  - NLP
  - R
tags:
  - NLP
  - Politics
---


```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


This project is intended to analyze the 2020 Democratic Presidential candidates' performances in the eleven debates. 

We'll start off by creating a tibble of the transcripts of the debates, with a row for each response given by a candidate. 

I scraped the transcripts from various sources, because no single source seemed to have transcripts for every debate. Unfortunately, that meant that the regex varied by debated, and so creating a function to automate this process would be much more difficult. 

```{r message=FALSE}
library(tidyverse)
library(rvest)
library(httr)
library(lubridate)
library(tidytext)
library(tidylo)
library(tidymodels)
library(textrecipes)
library(topicmodels)
library(vip)

```

```{r message=FALSE}
## Debate 1 pt 1 - Miami
html <- read_html("https://www.nytimes.com/2019/06/26/us/politics/democratic-debate-transcript.html")

debate1_1 <- html_nodes(html, ".StoryBodyCompanionColumn")
debate1_1 <- tibble(transcript = html_text(debate1_1, trim = TRUE))

debate1_1 <- debate1_1 %>%
    mutate(debate = 1) %>%
    separate_rows(transcript, sep = "[\\?\\.\\)\\’a-z](?=[A-Z]{2,})") %>%
    separate(transcript, sep = "\\:", into = "speaker", remove = FALSE)

```


I'll hide the rest of the scraping code since it's long and repetitive, but the idea is to repeat the above process for each debate, combine each dataframe, then do a little final cleaning.

```{r include=FALSE, cache=TRUE}

## Debate 1 pt 2 - Miami
html <- read_html("https://www.nytimes.com/2019/06/28/us/politics/transcript-debate.html")

debate1_2 <- html_nodes(html, ".StoryBodyCompanionColumn")
debate1_2 <- tibble(transcript = html_text(debate1_2, trim = TRUE))

debate1_2 <- debate1_2 %>%
    mutate(debate = 1) %>%
    separate_rows(transcript, sep = "[\\?\\.\\)\\’\\—a-z](?=[A-Z]{2,})") %>%
    separate(transcript, sep = "\\:", into = "speaker", remove = FALSE)

## Debate 2 pt 1 - Detroit
html <- read_html("https://www.rev.com/blog/transcripts/transcript-of-july-democratic-debate-night-1-full-transcript-july-30-2019")

debate2_1 <- html_nodes(html, "#transcription")
debate2_1 <- tibble(transcript = html_text(debate2_1, trim = TRUE))

debate2_1 <- debate2_1 %>%
    mutate(debate = 2) %>%
    separate_rows(transcript, sep = "[\\.\\?\\]\\’](?=[A-Z\\sa-z]+(W.)?:\\s\\()") %>%
    separate(transcript, sep = "\\:", into = "speaker", remove = FALSE)


## Debate 2 pt 2 - Detroit
html <- read_html("https://www.rev.com/blog/transcripts/transcript-of-july-democratic-debate-2nd-round-night-2-full-transcript-july-31-2019")

debate2_2 <- html_nodes(html, "#transcription")
debate2_2 <- tibble(transcript = html_text(debate2_2, trim = TRUE))

debate2_2 <- debate2_2 %>%
    mutate(debate = 2) %>%
    separate_rows(transcript, sep = "[\\.\\?\\]\\’](?=[A-Z\\sa-z]+(W.)?:\\s\\()") %>%
    separate(transcript, sep = "\\:", into = "speaker", remove = FALSE)


## Debate 3 - Houston
html <- read_html("https://www.rev.com/blog/transcripts/democratic-debate-transcript-houston-september-12-2019")

debate3 <- html_nodes(html, "#transcription")
debate3 <- tibble(transcript = html_text(debate3, trim = TRUE))

debate3 <- debate3 %>%
    mutate(debate = 3) %>%
    separate_rows(transcript, sep = "[\\.\\?\\’](?=[A-Z\\sa-z\\’]+:)") %>%
    separate_rows(transcript, sep = "[\\.\\?\\]](?=George S.:)") %>%
    separate(transcript, sep = "\\:", into = "speaker", remove = FALSE) %>%
    filter(transcript != "Beto O")


## Debate 4 - Ohio
html <- read_html("https://www.rev.com/blog/transcripts/october-democratic-debate-transcript-4th-debate-from-ohio")

debate4 <- html_nodes(html, "#transcription")
debate4 <- tibble(transcript = html_text(debate4, trim = TRUE))

debate4 <- debate4 %>%
    mutate(debate = 4) %>%
    separate_rows(transcript, sep = "[\\.\\?\\’\\-](?=[A-Z\\sa-z]+:)") %>%
    separate(transcript, sep = "\\:", into = "speaker", remove = FALSE) 

## Debate 5 - Atlanta
html <- read_html("https://www.nbcnews.com/politics/2020-election/read-democratic-debate-transcript-november-20-2019-n1088186")

debate5 <- html_nodes(html, ".article-body__content")
debate5 <- tibble(transcript = html_text(debate5, trim = TRUE))

debate5 <- debate5 %>%
    mutate(debate = 5) %>%
    separate_rows(transcript, sep = "[\\?\\.\\)\\’](?=[A-Z]{2,}:)") %>%
    separate(transcript, sep = "\\:", into = "speaker", remove = FALSE) 

## Debate 6 - LA
html <- read_html("https://www.rev.com/blog/transcripts/december-democratic-debate-transcript-sixth-debate-from-los-angeles")

debate6 <- html_nodes(html, ".fl-callout-content")
debate6 <- tibble(transcript = html_text(debate6, trim = TRUE))

debate6 <- debate6 %>%
    mutate(debate = 6) %>%
    separate_rows(transcript, sep = "[\\.\\?\\]](?=[A-Z\\sa-z]+(W.)?:\\s\\()") %>%
    separate(transcript, sep = "\\:", into = "speaker", remove = FALSE) 

## Debate 7 - Des Moines
html <- read_html("https://www.rev.com/blog/transcripts/january-iowa-democratic-debate-transcript")

debate7 <- html_nodes(html, ".fl-callout-content")
debate7 <- tibble(transcript = html_text(debate7, trim = TRUE))

debate7 <- debate7 %>%
    mutate(debate = 7) %>%
    separate_rows(transcript, sep = "[\\.\\?\\]](?=[A-Z\\sa-z\\d]+(W.)?:\\s\\()") %>%
    separate(transcript, sep = "\\:", into = "speaker", remove = FALSE) 

## Debate 8 - NH
html <- read_html("https://www.rev.com/blog/transcripts/new-hampshire-democratic-debate-transcript")

debate8 <- html_nodes(html, ".fl-callout-content")
debate8 <- tibble(transcript = html_text(debate8, trim = TRUE))

debate8 <- debate8 %>%
    mutate(debate = 8) %>%
    separate_rows(transcript, sep = "[\\.\\?\\]](?=[A-Z\\sa-z\\d]+(W.)?:\\s\\()") %>%
    separate_rows(transcript, sep = "[\\.\\?\\]](?=George S.:)") %>%
    separate(transcript, sep = "\\:", into = "speaker", remove = FALSE) 


## Debate 9 - Vegas
html <- read_html("https://www.nbcnews.com/politics/2020-election/full-transcript-ninth-democratic-debate-las-vegas-n1139546")

debate9 <- html_nodes(html, ".article-body__last-section")
debate9 <- tibble(transcript = html_text(debate9, trim = TRUE))

debate9 <- debate9 %>%
    mutate(debate = 9) %>%
    separate_rows(transcript, sep = "[\\?\\.\\)](?=[A-Z]{2,}:)") %>%
    separate(transcript, sep = "\\:", into = "speaker", remove = FALSE) 


## Debate 10 - SC
html <- read_html("https://www.rev.com/blog/transcripts/south-carolina-democratic-debate-transcript-february-democratic-debate")

debate10 <- html_nodes(html, "#transcription")
debate10 <- tibble(transcript = html_text(debate10, trim = TRUE))

debate10 <- debate10 %>%
    mutate(debate = 10) %>%
    separate_rows(transcript, sep = "[\\.\\?\\]\\-](?=[A-Z\\sa-z]+(W.)?:\\s\\()") %>%
    separate_rows(transcript, sep = "[\\.\\?\\]](?=Norah O’Donnell:)") %>%
    separate(transcript, sep = "\\:", into = "speaker", remove = FALSE) 

## Debate 11 - DC
html <- read_html("https://www.rev.com/blog/transcripts/march-democratic-debate-transcript-joe-biden-bernie-sanders")

debate11 <- html_nodes(html, "#transcription")
debate11 <- tibble(transcript = html_text(debate11, trim = TRUE))

debate11 <- debate11 %>%
    mutate(debate = 11) %>%
    separate_rows(transcript, sep = "[\\.\\?\\]\\-](?=[A-Z\\sa-z]+(W.)?:\\s\\()") %>%
    separate_rows(transcript, sep = "[\\.\\?\\]](?=Ilia Calderón:)") %>%
    separate(transcript, sep = "\\:", into = "speaker", remove = FALSE) 
## Combine all debates
all_debates <- bind_rows(debate1_1, debate1_2, debate2_1, debate2_2, debate3, debate4,
                         debate5, debate6, debate7, debate8, debate9, debate10, debate11) 
    

## Clean
candidates <- c("Elizabeth Warren", "Joe Biden", "Bill De Blasio", "Amy Klobuchar", 
                "Bernie Sanders", "Pete Buttigieg", "Marianne Williamson", "Michael Bennet", 
                "Jay Inslee", "John Hickenlooper", "Tom Steyer", "Eric Swalwell", 
                "Tulsi Gabbard", "Andrew Yang", "Kirsten Gillibrand",
                "Tim Ryan", "Julian Castro", "Steve Bullock", "Kamala Harris", 
                "Cory Booker", "John Delaney", "Mike Bloomberg", "Beto O'Rourke")

all_debates <- all_debates %>%
  mutate(speaker = str_trim(str_to_title(speaker)),
         speaker = if_else(str_detect(speaker, "Elizabeth|Warren"), "Elizabeth Warren", speaker),
         speaker = if_else(str_detect(speaker, "Biden"), "Joe Biden", speaker),
         speaker = if_else(str_detect(speaker, "Buttigieg"), "Pete Buttigieg", speaker),
         speaker = if_else(str_detect(speaker, "lasio"), "Bill De Blasio", speaker),
         speaker = if_else(str_detect(speaker, "Balart"), "Diaz Balart", speaker),
         speaker = if_else(str_detect(speaker, "Woodruff"), "Judy Woodruff", speaker),
         speaker = if_else(str_detect(speaker, "Cooper"), "Anderson Cooper", speaker),
         speaker = if_else(str_detect(speaker, "George"), "George S", speaker),
         speaker = if_else(str_detect(speaker, "Klobuchar"), "Amy Klobuchar", speaker),
         speaker = if_else(str_detect(speaker, "Sanders"), "Bernie Sanders", speaker),
         speaker = if_else(str_detect(speaker, "Marianne|Williamson"), "Marianne Williamson",
                           speaker),
         speaker = if_else(str_detect(speaker, "Bennet"), "Michael Bennet", speaker),
         speaker = if_else(str_detect(speaker, "Inslee"), "Jay Inslee", speaker),
         speaker = if_else(str_detect(speaker, "Hickenloop"), "John Hickenlooper", speaker),
         speaker = if_else(str_detect(speaker, "Steyer"), "Tom Steyer", speaker),
         speaker = if_else(str_detect(speaker, "Swalwell"), "Eric Swalwell", speaker),
         speaker = if_else(str_detect(speaker, "Gabbard"), "Tulsi Gabbard", speaker),
         speaker = if_else(str_detect(speaker, "Yang"), "Andrew Yang", speaker),
         speaker = if_else(str_detect(speaker, "Gilli"), "Kirsten Gillibrand", speaker),
         speaker = if_else(str_detect(speaker, "Ryan"), "Tim Ryan", speaker),
         speaker = if_else(str_detect(speaker, "Castro"), "Julian Castro", speaker),
         speaker = if_else(str_detect(speaker, "Bullock"), "Steve Bullock", speaker),
         speaker = if_else(str_detect(speaker, "Harris"), "Kamala Harris", speaker),
         speaker = if_else(str_detect(speaker, "Booker"), "Cory Booker", speaker),
         speaker = if_else(str_detect(speaker, "Delaney"), "John Delaney", speaker),
         speaker = if_else(str_detect(speaker, "Bloomberg"), "Mike Bloomberg", speaker),
         speaker = if_else(str_detect(speaker, "Rourke"), "Beto O'Rourke", speaker),
         candidate = if_else(speaker %in% candidates, 1, 0)) 

candidates_only <- all_debates %>%
    filter(candidate == 1) %>%
    mutate(transcript = str_remove(transcript, "^[\\w\\:]+\\s"),
           transcript = str_remove(transcript, "BLASIO: "),
           transcript = str_remove(transcript, "O$"),
           line_id = paste0("id_", row_number())) %>%
    select(-candidate)

candidates_only %>%
    head(5)

```

So now we've got a tibble called "candidates_only" with the transcript, the candidate, and the debate number. Here's what it looks like:

```{r}
candidates_only %>%
    glimpse()
```


# Words by candidate
Next we can use the tidytext package to transform the dataframe so that rows contain a single word spoken by that candidate. We'll remove stopwords using tidytext, and then remove a few additional words, including contractions and the names of candidates.

Then we can use the tidylo package to see the weighted log odds of each candidate using a particular word. 

```{r}

custom_stopwords <- c("it's", "biden", "steyer", "harris", "warren",
                      "buttigieg","klobuchar", "booker", "yang", "sanders", "that's",
                      "i'm", "it’s", "i’m", "01", "people", "12", "that’s", "we’re", 
                      "02", "don’t", "we’ve", "bloomberg", "time", "Senator",
                      "america", "country", "president", "bring", "american",
                      "pete", "bernie", "elizabeth", "mike", "cory", "joe",
                      "kamala", "amyklobuchar.com", "amy", "warre", 
                      "peteforamerica.com", "ilia", "calderón")

candidates_words <- candidates_only %>%
    unnest_tokens(word, transcript) %>%
    anti_join(stop_words, by = c("word" = "word")) %>%
    filter(!word %in% custom_stopwords) 



candidates_words %>%
    filter(speaker %in% c(
      "Joe Biden", "Bernie Sanders", "Elizabeth Warren", "Pete Buttigieg",
      "Amy Klobuchar", "Cory Booker", "Kamala Harris", 
      "Mike Bloomberg")) %>%
    add_count(speaker, name = "total_words") %>%
    group_by(speaker) %>%
    count(word, sort = TRUE) %>%
    mutate(word = str_remove_all(word, "[:punct:]")) %>%
    bind_log_odds(set = speaker, feature = word, n = n) %>%
    group_by(speaker) %>%
    top_n(8) %>%
    ungroup() %>%
    mutate(word = factor(word), 
           word = reorder_within(word, log_odds_weighted, speaker)) %>% 
    ggplot(aes(x = log_odds_weighted, y = word, fill = speaker)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~speaker, scales = "free_y") +
    scale_y_reordered() + 
    theme_minimal()



```

I'm pretty happy with how this turned out, as these seem to capture the primary messages of each candidate:


* Bernie Sanders' top word was, fittingly, "greed", likely directed at large corporations ("*It’s not just the price fixing and the corruption and the greed of the pharmaceutical industry, it’s what’s going on in the fossil fuel industry. It’s what’s going on in Wall Street. It’s what’s going on with the prison industrial complex. We need a mass political movement.*") "Industry" and "uninsured" shouldn't be surprising either, as they are likely used in his criticisms of the pharmaceutical and healthcare industries. 
* Cory Booker's campaign tried to focus on optimism, hope, and unity, and that is captured by "purpose" being his top word by a significant margin. 
* Elizabeth Warren's top words are strongly representative of her message of fighting for the middle class. "Giant" was her most distinctive word -- "*So I think of it this way, who is this economy really working for? It’s doing great for a thinner and thinner slice at the top. It’s doing great for giant drug companies.*" -- while her "two-**cent** wealth tax" made a lot of appearances as well. "Cancel" refers to her plan to cancel student loan debt.
* Biden made significant use of his experience as Vice-President, refering frequently to the broadly (and increasingly) popular "Obamacare" and his relationship with President Obama.
* Mike Bloomberg only joined the debates at the end, and his log odds words list mostly focused on mentions of New York City. The inclusion of "Senator" likely refers to his responses to Senator Warren. 


# Modeling which candidate said a particular line in the debates
OK, now let's build a model to predict which of the top 8 candidates said a particular line in the debates. Are the candidates' debate arguments distinct from one another enough to build a model around? We'll see! 

First, do a little more cleaning and EDA before we model. First, we want to filter out any short lines, like interruptions that only contain a few words. So we'll tokenize the text, add a column with word counts per debate line, and then filter for any below 25. 

Then we'll take a look at the distributions of word counts per candidate lines (which also shows who talked the most in the debates!).

Finally, we'll get the total number of lines spoken per candidate. Biden and Bernie have the most, in large part to their extra debate together. 

```{r}
library(tokenizers)


candidates_only2 <- candidates_only %>% 
    filter(speaker %in% c(
      "Joe Biden", "Bernie Sanders", "Elizabeth Warren", "Pete Buttigieg",
      "Amy Klobuchar", "Cory Booker", "Kamala Harris", 
      "Mike Bloomberg")) %>% 
    mutate(n_words = tokenizers::count_words(transcript)) %>%
    filter(n_words > 25)


## histogram
candidates_only2 %>%
    ggplot(aes(n_words, fill = speaker)) +
    geom_histogram() +
    theme(legend.position = "none") + 
    facet_wrap(~speaker) 


candidates_only2 %>%
    count(speaker) %>%
    ggplot(aes(n, reorder(speaker, n), fill = speaker)) +
    geom_col() +
    theme(legend.position = "none")
    

```

Now we can move on to the modeling itself. We'll create training and testing data, then define a preprocessing recipe that filters our stop words as well as the custom stopwords list I used above. I upsample because there's a significant difference between the number of lines spoken by Biden at the top and Bloomberg at the bottom. 

We'll start off with a regularized glmnet model, tuning for the number of tokens and the penalty used for regularization. It uses n-grams of two tokens as well. I also did an xgboost model, but I won't actually run that code for this post, because it took nearly 25 hours to run *shocked emoji* and actually performed a little worse than the glmnet model below.

```{r}

candidates_only3 <- candidates_only2 %>%
    select(line_id, speaker, transcript)

set.seed(123)
candidates_split <- initial_split(candidates_only3)
candidates_train <- training(candidates_split)
candidates_test <- testing(candidates_split)
candidates_folds <- vfold_cv(candidates_train, folds = 10, strata = speaker)


glmnet_recipe <- recipe(speaker ~ ., data = candidates_train) %>%
    update_role(line_id, new_role = "id") %>%
    step_string2factor(speaker) %>%
    step_tokenize(transcript) %>%
    step_stopwords(transcript) %>%
    step_stopwords(transcript, custom_stopword_source = custom_stopwords) %>% 
    step_ngram(transcript, num_tokens = 2, min_num_tokens = 1) %>%
    step_tokenfilter(transcript, max_tokens = tune::tune(), min_times = 10) %>%
    step_tfidf(transcript) %>%
    step_normalize(recipes::all_predictors()) %>%
    themis::step_upsample(speaker)

glmnet_spec <-  multinom_reg(penalty = tune(), mixture = 1) %>% 
    set_mode("classification") %>% 
    set_engine("glmnet") 

glmnet_workflow <- workflow() %>% 
    add_recipe(glmnet_recipe) %>% 
    add_model(glmnet_spec) 

glmnet_grid <- grid_max_entropy(
    penalty(range = c(-4, 0)),
    max_tokens(),
    size = 25)

all_cores <- parallel::detectCores(logical = FALSE)
cl <- parallel::makePSOCKcluster(all_cores)
doParallel::registerDoParallel(cl)
set.seed(1234)

glmnet_tune <- tune_grid(
    glmnet_workflow, 
    resamples = candidates_folds, 
    grid = glmnet_grid,
    metrics = metric_set(accuracy, roc_auc),
    control = control_grid(save_pred = TRUE, pkgs = c('textrecipes'))) 


```


Here's the code for the glmnet, in case you're interested:

```{r eval = FALSE}
xgboost_recipe <- recipe(formula = speaker ~ ., data = candidates_train) %>% 
    update_role(line_id, new_role = "id") %>%
    step_string2factor(speaker) %>%
    step_tokenize(transcript) %>%
    step_stopwords(transcript) %>%
    step_stopwords(transcript, custom_stopword_source = custom_stopwords) %>% 
    step_ngram(transcript, num_tokens = 2, min_num_tokens = 1) %>%
    step_tokenfilter(transcript, max_tokens = tune::tune(), min_times = 10) %>%
    step_tfidf(transcript) %>%
    step_normalize(recipes::all_predictors()) %>%
    themis::step_upsample(speaker)


xgboost_spec <- boost_tree(
    trees = 1000,
    min_n = tune(), 
    tree_depth = tune(), 
    learn_rate = tune(), 
    loss_reduction = tune(), 
    sample_size = tune()) %>% 
    set_mode("classification") %>% 
    set_engine("xgboost") 



xgb_grid <- xgboost_spec %>%
    parameters() %>%
    grid_latin_hypercube(size = 20)


xgboost_workflow <- workflow() %>% 
    add_recipe(xgboost_recipe) %>% 
    add_model(xgboost_spec) 

all_cores <- parallel::detectCores(logical = FALSE)
cl <- parallel::makePSOCKcluster(all_cores)
doParallel::registerDoParallel(cl)

tictoc::tic()
set.seed(72008)

xgboost_tune <- tune_race_anova(
    xgboost_workflow,
    resamples = candidates_folds,
    grid = xgb_grid,
    metrics = metric_set(accuracy, roc_auc),
    control = control_grid(save_pred = TRUE, pkgs = c("textrecipes")))


tictoc::toc()
beepr::beep(2)


show_best(xgboost_tune, "accuracy")
show_best(xgboost_tune, "roc_auc")

xgb_pred <- collect_predictions(xgboost_tune)

xgb_pred %>%
    filter(id == "Fold01") %>%
    conf_mat(speaker, .pred_class) %>%
    autoplot(type = "heatmap")



```





```{r}
show_best(glmnet_tune, "accuracy")
show_best(glmnet_tune, "roc_auc")

lasso_pred <- collect_predictions(glmnet_tune)


best_roc <- select_best(glmnet_tune, "roc_auc")
lasso_wf_final <- finalize_workflow(glmnet_workflow, best_roc)


## variable importance
library(vip)

lasso_wf_final %>%
    fit(candidates_train) %>%
    pull_workflow_fit() %>%
    vi(lambda = best_roc$penalty) %>%
    slice_head(n = 25) %>%
    mutate(Importance = abs(Importance),
           Variable = str_remove(Variable, "tfidf_transcript_"),
           Variable = fct_reorder(Variable, Importance)) %>%
    ggplot(aes(Importance, Variable, fill = Sign)) +
    geom_col() +
    theme_classic() +
    labs(x = NULL, y = NULL)

```

So, not *amazing* performance, but this is a difficult problem with 8 classes and not a ton of observations. 

But the variable importance plot is interesting. "Thank (you,) Senator" is the most important n-gram, followed closely by "think". "Think" was used overwhelmingly by Amy Klobuchar (98 times), followed by Bernie (81). 

``` {r}
## final model, evaluate with test data
final_res <- lasso_wf_final %>%
    last_fit(candidates_split, metrics = metric_set(accuracy, roc_auc))

collect_metrics(final_res)


final_res %>%
    collect_predictions() %>%
    conf_mat(speaker, .pred_class) %>%
    autoplot(type = "heatmap") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))



```

The good news is that we didn't overfit, even if our test accuracy was still just 57%. But the confusion matrix was pretty solid nevertheless, mostly getting the right person for each test line. 


# What did candidates talk about? 
We can also do a little LDA to see what the candidates talked about. We cast our previous dataframe as a document-term matrix, then run LDA, collecting the beta probabilities for words in each of the three topics we'll look at. 

```{r}

candidates_dtm <- candidates_only3 %>%
    rename(text = transcript) %>%
    unnest_tokens(word, text) %>%
    anti_join(stop_words, by = "word") %>%
    anti_join(tibble(word = custom_stopwords)) %>%
    count(speaker, word, sort = TRUE) %>%
    cast_dtm(speaker, word, n)

candidates_lda <- LDA(candidates_dtm, k = 3, control = list(seed = 123))



candidates_topics <- candidates_lda %>%
    tidy(matrix = "beta")


candidates_top_terms <- candidates_topics %>%
    group_by(topic) %>%
    top_n(10, abs(beta)) %>%
    ungroup() %>%
    arrange(topic, desc(beta))


candidates_top_terms %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free") +
    scale_y_reordered()

```

I don't know exactly what I expected, but there don't seem to be huge differences between the three topics. "Plan", "Trump", "healthcare", and "united" are common to all topics.

* However, topic one might be the Elizabeth Warren topic, as it's distinguished by words like "I have a **plan** for that", "healthcare", "pay" and "money", and "fight." 
* Topic two might be about Trump and Democrats' arguments about his failures in health insurance policy. 
* Topic three is also about Trump, but seems to me to be more Biden-ish, with words like "deal", "world", and "bill". 



